{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Kernel \n",
    "\n",
    "In this code sample, you will use the [Semantic Kernel](https://aka.ms/ai-agents-beginners/semantic-kernel) AI Framework to create a basic agent. \n",
    "\n",
    "The goal of this sample is to show you the steps that we will later use in the addtional code samples when implementing the different agentic patterns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Needed Python Packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from typing import Annotated\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "\n",
    "from semantic_kernel.kernel import Kernel\n",
    "from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion\n",
    "from semantic_kernel.agents import ChatCompletionAgent\n",
    "from semantic_kernel.contents import ChatHistory\n",
    "\n",
    "\n",
    "from semantic_kernel.agents.open_ai import OpenAIAssistantAgent\n",
    "from semantic_kernel.contents import AuthorRole, ChatMessageContent\n",
    "from semantic_kernel.functions import kernel_function\n",
    "\n",
    "from semantic_kernel.connectors.ai import FunctionChoiceBehavior\n",
    "\n",
    "from semantic_kernel.contents.function_call_content import FunctionCallContent\n",
    "from semantic_kernel.contents.function_result_content import FunctionResultContent\n",
    "from semantic_kernel.functions import KernelArguments, kernel_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Client and Kernel \n",
    "\n",
    "In this sample, we will use [GitHub Models](https://aka.ms/ai-agents-beginners/github-models) for access to the LLM. \n",
    "\n",
    "The `ai_model_id` is defined as `gpt-4o-mini`. Try changing the model to another model available on the GitHub Models marketplace to see the different results. \n",
    "\n",
    "For us to us the `Azure Inference SDK` that is used for the `base_url` for GitHub Models, we will use the `AsyncOpenAI` connector within Semantic Kernel. There are also other [available connectors](https://learn.microsoft.com/semantic-kernel/concepts/ai-services/chat-completion) to use Semantic Kernel for other model providers.\n",
    "\n",
    "We will also create a `Kernel`. A `kernel` is a collection of the services and plugins that will be used by your Agents. In this snipppet, we are creating the kernel and adding the `chat_completion_service` to it.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random   \n",
    "\n",
    "# Define a sample plugin for the sample\n",
    "class DestinationsPlugin:\n",
    "    \"\"\"A List of Random Destinations for a vacation.\"\"\"\n",
    "\n",
    "    # The __init__ method you shared is indeed a constructor for the DestinationsPlugin class. \n",
    "    # In Python, __init__ serves as the class constructor and is automatically called when you\n",
    "    # create a new instance of the class.\n",
    "    def __init__(self):\n",
    "        # List of vacation destinations\n",
    "        self.destinations = [\n",
    "            \"Barcelona, Spain\",\n",
    "            \"Paris, France\",\n",
    "            \"Berlin, Germany\",\n",
    "            \"Tokyo, Japan\",\n",
    "            \"Sydney, Australia\",\n",
    "            \"New York, USA\",\n",
    "            \"Cairo, Egypt\",\n",
    "            \"Cape Town, South Africa\",\n",
    "            \"Rio de Janeiro, Brazil\",\n",
    "            \"Bali, Indonesia\"\n",
    "        ]\n",
    "        # Track last destination to avoid repeats\n",
    "        self.last_destination = None\n",
    "\n",
    "    # @kernel_function is a decorator from the Semantic Kernel framework that transforms a regular \n",
    "    # Python method into a function that can be discovered and called by AI agents.\n",
    "    #   Registers the function with Semantic Kernel so it's available to agents\n",
    "    #   Provides metadata about the function's purpose and behavior\n",
    "    #   Enables function calling by the AI model through the kernel#\n",
    "    @kernel_function(description=\"Provides a random vacation destination.\") \n",
    "    def get_random_destination(self) -> Annotated[str, \"Returns a random vacation destination.\"]: \n",
    "        # An annotation is used to provide type hints (a string) and metadata (description) for the function's return value.\n",
    "        # Get available destinations (excluding last one if possible)\n",
    "        available_destinations = self.destinations.copy()\n",
    "        if self.last_destination and len(available_destinations) > 1:\n",
    "            available_destinations.remove(self.last_destination)\n",
    "\n",
    "        # Select a random destination\n",
    "        destination = random.choice(available_destinations)\n",
    "\n",
    "        # Update the last destination\n",
    "        self.last_destination = destination\n",
    "\n",
    "        return destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_dotenv() loads environment variables from .env file, \n",
    "#  making credentials available while keeping them secure and out of source code\n",
    "load_dotenv()\n",
    "\n",
    "# Creates AsyncOpenAI client to connect to Azure-hosted models\n",
    "# Uses GITHUB_TOKEN from environment variables for authentication\n",
    "# Points to Azure's inference endpoint via base_url\n",
    "client = AsyncOpenAI(\n",
    "    api_key=os.environ.get(\"GITHUB_TOKEN\"), base_url=\"https://models.inference.ai.azure.com/\")\n",
    "\n",
    "# Instantiates the Kernel - the central orchestration component\n",
    "# Registers DestinationsPlugin with a logical name (\"destinations\") for function discovery\n",
    "kernel = Kernel()\n",
    "kernel.add_plugin(DestinationsPlugin(), plugin_name=\"destinations\")\n",
    "\n",
    "# Defines service_id (\"agent\") to identify a specific LM service instance. Doing so \n",
    "# enables the system to reference the LM service elsewhere in the code.\n",
    "service_id = \"agent\"\n",
    "\n",
    "#   Creates instance of OpenAIChatCompletion, which acts as a service\n",
    "chat_completion_service = OpenAIChatCompletion(\n",
    "    ai_model_id=\"gpt-4o-mini\", # gpt-4o-mini as the model\n",
    "    async_client=client, # previously created async client\n",
    "    service_id=service_id # creates OpenAIChatCompletion service using:\n",
    ")\n",
    "\n",
    "# Adds the service to the kernel, making it available to the agent\n",
    "kernel.add_service(chat_completion_service)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Agent \n",
    "\n",
    "Below we will are creating the Agent called `TravelAgent` and also creating a variable called `AGENT_INSTRUCTIONS`. We will later add this to our `system_message` that will give the agent instructions on the task, behavior and tone.\n",
    "\n",
    "For this example, we are using very simple instructions. You can change these instructions to see how the agent responds differently. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This configuration is what enables the natural function calling seen later when users ask about \n",
    "# travel destinations. The agent autonomously decides when to use the destination function.\n",
    "# This example, the user never needs to know the function exists - they just ask for travel plans, \n",
    "# and the agent intelligently uses its available tools.\n",
    "\n",
    "# Fetches configuration settings for the specified service\n",
    "# Uses the previously defined service_id (\"agent\") to retrieve the right settings\n",
    "settings = kernel.get_prompt_execution_settings_from_service_id(\n",
    "    service_id=service_id)\n",
    "\n",
    "# Enables AI-driven function calling\n",
    "# Instructs the LLM to automatically determine when to call functions (rather than requiring explicit instructions)\n",
    "# Allows the agent to independently decide when to use get_random_destination() based on conversation context\n",
    "settings.function_choice_behavior = FunctionChoiceBehavior.Auto()\n",
    "\n",
    "# 'FunctionChoiceBehavior.Auto()' - This setting tells Semantic Kernel to let the LLM (in this case GPT-4o-mini) \n",
    "# decide when to call functions on its own, rather than requiring direct instructions. Here's what this means: \n",
    "#\n",
    "# Without Auto Function Calling:\n",
    "#  Developer would need to explicitly code when to call functions\n",
    "#  Or users would need to explicitly ask for specific functions (\"get me a random destination\")\n",
    "#  Leads to unnatural interactions and more complex code\n",
    "#\n",
    "# With Auto Function Calling:\n",
    "#  The AI model analyzes user requests like \"Plan me a day trip\"\n",
    "#  It recognizes when a registered function would be helpful\n",
    "#  It autonomously chooses to call get_random_destination() when appropriate\n",
    "#  The kernel handles the actual function execution\n",
    "#\n",
    "#  This creates a more natural conversation flow where the travel agent can seamlessly integrate function outputs \n",
    "#  (destination suggestions) into its responses based on contextual understanding of the conversation.\n",
    "#\n",
    "#  In this example, the user never needs to know the function exists - they just ask for travel plans,\n",
    "#  and the agent intelligently uses its available tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AGENT_NAME = \"TravelAgent\"\n",
    "AGENT_INSTRUCTIONS = \"You are a helpful AI Agent that can help plan vacations for customers at random destinations\"\n",
    "\n",
    "# ChatCompletionAgent is a specialized SK agent that uses chat-based interactions to communicate with users.\n",
    "# SK encapsulates the agent's behavior, instructions, and settings into a single object.\n",
    "# This class creates an agent that can:\n",
    "#   Process natural language through the connected language model\n",
    "#   Call functions autonomously based on the conversation context\n",
    "#   Maintain conversation state across multiple turns\n",
    "#   Follow system instructions defined in AGENT_INSTRUCTIONS\n",
    "agent = ChatCompletionAgent(\n",
    "    service_id=service_id, \n",
    "    kernel=kernel, \n",
    "    name=AGENT_NAME,\n",
    "    instructions=AGENT_INSTRUCTIONS,\n",
    "    arguments=KernelArguments(settings=settings)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Agents \n",
    "\n",
    "Now we can run the Agent by defining the `ChatHistory` and adding the `system_message` to it. We will use the `AGENT_INSTRUCTIONS` that we defined earlier. \n",
    "\n",
    "After these are defined, we create a `user_inputs` that will be what the user is sending to the agent. In this case, we have set this message to `Plan me a sunny vacation`. \n",
    "\n",
    "Feel free to change this message to see how the agent responds differently. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML  # Import utilities to render HTML in Jupyter notebooks\n",
    "\n",
    "async def main():\n",
    "    # --- CHAT HISTORY INITIALIZATION ---\n",
    "    # External History Management: The history object is created and managed outside the agent\n",
    "    #   Explicit Additions: You choose when to add user messages\n",
    "    #   Pre-processing: You can modify the history before passing it to the agent\n",
    "    #      You could implement a \"Sliding Window with Decay\" Pattern\"\n",
    "    #   Complete Replacement: You could substitute a custom history implementation\n",
    "    chat_history = ChatHistory()  # Create empty conversation container\n",
    "    \n",
    "    # --- USER INPUT DEFINITION ---\n",
    "    # Define multiple messages to simulate a multi-turn conversation\n",
    "    # This tests both initial response and contextual follow-up handling\n",
    "    user_inputs = [\n",
    "        \"Plan me a day trip.\",  # Initial query that likely triggers destination function\n",
    "        \"I don't like that destination. Plan me another vacation.\",  # Follow-up showing contextual awareness\n",
    "    ]\n",
    "    \n",
    "    # --- PROCESS EACH USER MESSAGE ---\n",
    "    # Iterate through each message to simulate a conversation\n",
    "    for user_input in user_inputs:\n",
    "        # Add current message to conversation history\n",
    "        # This updates the context that will be sent to the LLM\n",
    "        chat_history.add_user_message(user_input)  # Adds with \"user\" role automatically\n",
    "        \n",
    "        # --- HTML RENDERING SETUP (USER MESSAGE) ---\n",
    "        # Start constructing the HTML display for this conversation turn\n",
    "        # Create container div with bottom margin\n",
    "        html_output = f\"<div style='margin-bottom:10px'>\"\n",
    "        # Add bold \"User:\" label\n",
    "        html_output += f\"<div style='font-weight:bold'>User:</div>\"\n",
    "        # Add user's message with indentation for readability\n",
    "        html_output += f\"<div style='margin-left:20px'>{user_input}</div>\"\n",
    "        html_output += f\"</div>\"  # Close the user message container\n",
    "        \n",
    "        # --- RESPONSE TRACKING VARIABLES ---\n",
    "        agent_name: str | None = None  # Will store agent name when available\n",
    "        full_response = \"\"  # Accumulates text portions of the response\n",
    "        function_calls = []  # Tracks function call events and results\n",
    "        function_results = {}  # Maps function names to their returned values\n",
    "        \n",
    "        # --- STREAM PROCESSING LOOP ---\n",
    "        # Process chunks from the streaming response\n",
    "        # Key to function calling: we process content incrementally as it arrives\n",
    "        # Agent Invocation with the current conversation context through Streaming API\n",
    "        \n",
    "        # The 'async for' loop processes each chunk non-blockingly, which is essential when:\n",
    "        #   Working with potentially slow network requests\n",
    "        #   Handling function execution that might take time\n",
    "        #   Maintaining responsiveness in interactive applications\n",
    "\n",
    "        # The 'invoke_stream()' method returns an asynchronous iterator that:\n",
    "        #   Produces response chunks as they become available from the LLM\n",
    "        #   Allows for real-time processing of content without waiting for the complete response\n",
    "        #   Enables immediate reaction to LLM decisions\n",
    "        async for content in agent.invoke_stream(chat_history):\n",
    "            # Extract agent name if available and not already captured\n",
    "            if not agent_name and hasattr(content, 'name'):\n",
    "                agent_name = content.name  # Store name for display\n",
    "            \n",
    "            # --- FUNCTION CALL DETECTION AND HANDLING ---\n",
    "            # Process each content item to detect function calls and results\n",
    "            for item in content.items:\n",
    "                # CASE 1: LLM has decided to call a function\n",
    "                if isinstance(item, FunctionCallContent):\n",
    "                    # FunctionCallContent contains:\n",
    "                    # - function_name: which registered function to call\n",
    "                    # - arguments: parameters to pass to the function (as dict/JSON)\n",
    "                    call_info = f\"Calling: {item.function_name}({item.arguments})\"\n",
    "                    # Add call to tracking list for UI display\n",
    "                    function_calls.append(call_info)\n",
    "                     # The function execution happens HERE â†“ but is invisible in the code\n",
    "                     # When Semantic Kernel sees FunctionCallContent, it:\n",
    "                     #   1. Looks up the registered function (\"destinations.get_random_destination\")\n",
    "                     #   2. Executes it with the provided arguments\n",
    "                     #   3. Captures the return value\n",
    "                     # Function execution happens automatically via kernel within the framework pipeline\n",
    "                \n",
    "                # CASE 2: Detects that a function has returned results\n",
    "                elif isinstance(item, FunctionResultContent):\n",
    "                    # FunctionResultContent contains:\n",
    "                    # - function_name: which function produced this result\n",
    "                    # - result: the actual return value from the function\n",
    "                    # Format the function result for display\n",
    "                    result_info = f\"Result: {item.result}\"\n",
    "                    # Add the result to the tracking list\n",
    "                    function_calls.append(result_info)\n",
    "                    # Store result in dictionary for potential later use\n",
    "                    # Key = function name, Value = return value\n",
    "                    function_results[item.function_name] = item.result\n",
    "                    # Behind the scenes:\n",
    "                    # 1. Function was executed by SK framework\n",
    "                    # 2. Result is captured and fed back to the LLM\n",
    "                    # 3. LLM incorporates result into ongoing response generation\n",
    "            \n",
    "            # --- TEXT RESPONSE EXTRACTION ---\n",
    "            # Process text content from the agent (not function-related messages)\n",
    "            # Add text content to response if all conditions are met:\n",
    "            if (hasattr(content, 'content') and  # Has content attribute\n",
    "                content.content and  # Content is not None\n",
    "                content.content.strip() and  # Content is not just whitespace\n",
    "                # Content is not from function calls/results\n",
    "                not any(isinstance(item, (FunctionCallContent, FunctionResultContent))\n",
    "                      for item in content.items)):\n",
    "                # Add this chunk to the accumulated response text\n",
    "                full_response += content.content  # Append text to full response\n",
    "        \n",
    "        # --- FUNCTION CALL DISPLAY GENERATION ---\n",
    "        # If any functions were called, create a collapsible UI section to show them\n",
    "        if function_calls:\n",
    "            html_output += f\"<div style='margin-bottom:10px'>\"\n",
    "            html_output += f\"<details>\"  # HTML tag for expandable content\n",
    "            # Create clickable summary header for the details\n",
    "            html_output += f\"<summary style='cursor:pointer; font-weight:bold; color:#0066cc;'>Function Calls (click to expand)</summary>\"\n",
    "            # Create styled container for function call logs\n",
    "            html_output += f\"<div style='margin:10px; padding:10px; background-color:#f8f8f8; border:1px solid #ddd; border-radius:4px; white-space:pre-wrap;'>\"\n",
    "            # Join all function call logs with line breaks\n",
    "            html_output += \"<br>\".join(function_calls)\n",
    "            html_output += f\"</div></details></div>\"  # Close containers\n",
    "        \n",
    "        # --- AGENT RESPONSE DISPLAY GENERATION ---\n",
    "        # Create container for agent's text response\n",
    "        html_output += f\"<div style='margin-bottom:20px'>\"\n",
    "        # Add agent name (or fallback to 'Assistant')\n",
    "        html_output += f\"<div style='font-weight:bold'>{agent_name or 'Assistant'}:</div>\"\n",
    "        # Add formatted agent response with indentation and whitespace preservation\n",
    "        html_output += f\"<div style='margin-left:20px; white-space:pre-wrap'>{full_response}</div>\"\n",
    "        html_output += f\"</div>\"  # Close agent response container\n",
    "        html_output += \"<hr>\"  # Add horizontal rule between conversation turns\n",
    "        \n",
    "        # --- RENDER OUTPUT ---\n",
    "        # Convert HTML string to renderable HTML\n",
    "        # Render the final HTML output in the Jupyter notebook\n",
    "        display(HTML(html_output))  # Jupyter display function\n",
    "\n",
    "# --- EXECUTE MAIN FUNCTION ---\n",
    "# Call the async main function and await its completion\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "async def main():\n",
    "    # Define the chat history\n",
    "    # External History Management: The history object is created and managed outside the agent\n",
    "    #   Explicit Additions: You choose when to add user messages\n",
    "    #   Pre-processing: You can modify the history before passing it to the agent\n",
    "    #      You could implement a \"Sliding Window with Decay\" Pattern\"\n",
    "    #   Complete Replacement: You could substitute a custom history implementation\n",
    "    chat_history = ChatHistory()\n",
    "\n",
    "    # Respond to user input\n",
    "    user_inputs = [\n",
    "        \"Plan me a day trip.\",\n",
    "        \"I don't like that destination. Plan me another vacation.\",\n",
    "    ]\n",
    "\n",
    "    for user_input in user_inputs:\n",
    "        # Add the user input to the chat history\n",
    "        chat_history.add_user_message(user_input)\n",
    "\n",
    "        # Start building HTML output\n",
    "        html_output = f\"<div style='margin-bottom:10px'>\"\n",
    "        html_output += f\"<div style='font-weight:bold'>User:</div>\"\n",
    "        html_output += f\"<div style='margin-left:20px'>{user_input}</div>\"\n",
    "        html_output += f\"</div>\"\n",
    "\n",
    "        agent_name: str | None = None\n",
    "        full_response = \"\"\n",
    "        function_calls = []\n",
    "        function_results = {}\n",
    "\n",
    "        # Collect the agent's response with function call tracking\n",
    "        async for content in agent.invoke_stream(chat_history):\n",
    "            if not agent_name and hasattr(content, 'name'):\n",
    "                agent_name = content.name\n",
    "\n",
    "            # Track function calls and results\n",
    "            for item in content.items:\n",
    "                if isinstance(item, FunctionCallContent):\n",
    "                    call_info = f\"Calling: {item.function_name}({item.arguments})\"\n",
    "                    function_calls.append(call_info)\n",
    "                elif isinstance(item, FunctionResultContent):\n",
    "                    result_info = f\"Result: {item.result}\"\n",
    "                    function_calls.append(result_info)\n",
    "                    # Store function results\n",
    "                    function_results[item.function_name] = item.result\n",
    "\n",
    "            # Add content to response if it's not a function-related message\n",
    "            if (hasattr(content, 'content') and content.content and content.content.strip() and\n",
    "                not any(isinstance(item, (FunctionCallContent, FunctionResultContent))\n",
    "                        for item in content.items)):\n",
    "                full_response += content.content\n",
    "\n",
    "        # Add function calls to HTML if any occurred\n",
    "        if function_calls:\n",
    "            html_output += f\"<div style='margin-bottom:10px'>\"\n",
    "            html_output += f\"<details>\"\n",
    "            html_output += f\"<summary style='cursor:pointer; font-weight:bold; color:#0066cc;'>Function Calls (click to expand)</summary>\"\n",
    "            html_output += f\"<div style='margin:10px; padding:10px; background-color:#f8f8f8; border:1px solid #ddd; border-radius:4px; white-space:pre-wrap;'>\"\n",
    "            html_output += \"<br>\".join(function_calls)\n",
    "            html_output += f\"</div></details></div>\"\n",
    "\n",
    "        # Add agent response to HTML\n",
    "        html_output += f\"<div style='margin-bottom:20px'>\"\n",
    "        html_output += f\"<div style='font-weight:bold'>{agent_name or 'Assistant'}:</div>\"\n",
    "        html_output += f\"<div style='margin-left:20px; white-space:pre-wrap'>{full_response}</div>\"\n",
    "        html_output += f\"</div>\"\n",
    "        html_output += \"<hr>\"\n",
    "\n",
    "        # Display formatted HTML\n",
    "        display(HTML(html_output))\n",
    "\n",
    "await main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-begin2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
